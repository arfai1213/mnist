{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parsePoint(line):\n",
    "    values = [int(x) for x in line.split(\",\")]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "training_data = sc.textFile('./mnist/mnist_train.csv')\\\n",
    "        .mapPartitionsWithIndex(\n",
    "            lambda idx, it: islice(it, 1, None) if idx == 0 else it \n",
    "        )\\\n",
    "        .map(parsePoint)\n",
    "\n",
    "test_data = sc.textFile('./mnist/mnist_test.csv')\\\n",
    "        .mapPartitionsWithIndex(\n",
    "            lambda idx, it: islice(it, 1, None) if idx == 0 else it \n",
    "        )\\\n",
    "        .map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithLBFGS.train(training_data, regParam=0, numClasses=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "752"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.map(lambda d:\n",
    "    (d.label, model.predict(d.features), model.predict(d.features) == int(d.label))\n",
    " ).filter(lambda d: d[2] == False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.map(lambda d:\n",
    "    (d.label, model.predict(d.features), model.predict(d.features) == int(d.label))\n",
    " ).filter(lambda d: d[2] == False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = test_data.map(lambda d:\n",
    "    (float(model.predict(d.features)), d.label)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 0.9248\n",
      "Recall = 0.9248\n",
      "F1 Score = 0.9248\n"
     ]
    }
   ],
   "source": [
    "# Overall statistics\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0.0 precision = 0.9561316051844466\n",
      "Class 0.0 recall = 0.9785714285714285\n",
      "Class 0.0 F1 Measure = 0.967221381744831\n",
      "Class 1.0 precision = 0.9660869565217391\n",
      "Class 1.0 recall = 0.9788546255506608\n",
      "Class 1.0 F1 Measure = 0.9724288840262582\n",
      "Class 2.0 precision = 0.936734693877551\n",
      "Class 2.0 recall = 0.8895348837209303\n",
      "Class 2.0 F1 Measure = 0.9125248508946321\n",
      "Class 3.0 precision = 0.90234375\n",
      "Class 3.0 recall = 0.9148514851485149\n",
      "Class 3.0 F1 Measure = 0.9085545722713865\n",
      "Class 4.0 precision = 0.934560327198364\n",
      "Class 4.0 recall = 0.9307535641547862\n",
      "Class 4.0 F1 Measure = 0.9326530612244899\n",
      "Class 5.0 precision = 0.9090909090909091\n",
      "Class 5.0 recall = 0.8632286995515696\n",
      "Class 5.0 F1 Measure = 0.8855664174813112\n",
      "Class 6.0 precision = 0.9334016393442623\n",
      "Class 6.0 recall = 0.9509394572025052\n",
      "Class 6.0 F1 Measure = 0.9420889348500517\n",
      "Class 7.0 precision = 0.9269005847953217\n",
      "Class 7.0 recall = 0.9250972762645915\n",
      "Class 7.0 F1 Measure = 0.9259980525803312\n",
      "Class 8.0 precision = 0.8615232443125618\n",
      "Class 8.0 recall = 0.8942505133470225\n",
      "Class 8.0 F1 Measure = 0.8775818639798487\n",
      "Class 9.0 precision = 0.9144278606965174\n",
      "Class 9.0 recall = 0.910802775024777\n",
      "Class 9.0 F1 Measure = 0.9126117179741807\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Statistics by class\n",
    "labels = pred_labels.map(lambda lp: lp[1]).distinct().collect()\n",
    "for label in sorted(labels):\n",
    "    print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "    print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "    print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted recall = 0.9248000000000001\n",
      "Weighted precision = 0.9249076315597062\n",
      "Weighted F(1) Score = 0.9246777208704127\n",
      "Weighted F(0.5) Score = 0.9247725529819946\n",
      "Weighted false positive rate = 0.008289218806257687\n"
     ]
    }
   ],
   "source": [
    "# Weighted stats\n",
    "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
